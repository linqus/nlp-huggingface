{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnLNEG0swdtR/9K47ATPh7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linqus/nlp-huggingface/blob/main/notebooks/unit2/nlp_2_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy1nHSj9ixYb",
        "outputId": "8487dca6-1789-413c-b85e-15db137cd692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.1)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2023.11.17)\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "hUbH2SX4i4f9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I like icecreams too\"\n",
        "]\n",
        "prompt =\"I've been waiting for a HuggingFace course my whole life.\"\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokens = tokenizer.tokenize(sequences)\n",
        "print(\"raw tokens: \",tokens)\n",
        "output = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"converted ids: \", ids)\n",
        "\n",
        "tensor = torch.tensor(ids)\n",
        "print(\"tensor without dim: \", tensor)\n",
        "\n",
        "tensor_dim = torch.tensor([ids])\n",
        "print(\"tensor with dimention: \", tensor_dim)\n",
        "\n",
        "model_input = tokenizer(prompt, return_tensors=\"pt\")\n",
        "print(\"from tokenizer: \", model_input['input_ids'])\n",
        "\n",
        "model = AutoModel.from_pretrained(checkpoint)\n",
        "output = model(tensor_dim)\n",
        "print(\"output from tensor with dim: \", output)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "output = model(tensor_dim)\n",
        "print(\"classificator logit output: \", output[\"logits\"])\n",
        "\n",
        "\n",
        "batched_ids = [ids, ids]\n",
        "tensor_batched = torch.tensor(batched_ids)\n",
        "output = model(tensor_batched)\n",
        "print(\"classificator logit output (batched): \", output[\"logits\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWj1yAwOi-9-",
        "outputId": "a06e5c16-0a44-46bb-95c3-994fcf7d91b1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw tokens:  ['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.', 'i', 'like', 'ice', '##cre', '##ams', 'too']\n",
            "converted ids:  [1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 1045, 2066, 3256, 16748, 13596, 2205]\n",
            "tensor without dim:  tensor([ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "         2026,  2878,  2166,  1012,  1045,  2066,  3256, 16748, 13596,  2205])\n",
            "tensor with dimention:  tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012,  1045,  2066,  3256, 16748, 13596,  2205]])\n",
            "from tokenizer:  tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102]])\n",
            "output from tensor with dim:  BaseModelOutput(last_hidden_state=tensor([[[-0.4245,  0.4682,  0.4638,  ..., -0.0660,  0.0738, -0.0355],\n",
            "         [-0.1034,  0.6148,  0.6247,  ..., -0.1498,  0.2902, -0.5262],\n",
            "         [ 0.0747,  0.7071,  0.0687,  ..., -0.6356,  0.3194,  0.3122],\n",
            "         ...,\n",
            "         [-0.4167,  1.0581,  0.3836,  ..., -0.3581,  0.0357, -0.1862],\n",
            "         [-0.7297,  0.6921,  0.0800,  ..., -0.0444, -0.1177, -0.3495],\n",
            "         [-0.5737,  0.5894,  0.1252,  ..., -0.2639, -0.1625, -0.2014]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n",
            "classificator logit output:  tensor([[ 0.3482, -0.1162]], grad_fn=<AddmmBackward0>)\n",
            "classificator logit output (batched):  tensor([[ 0.3482, -0.1162],\n",
            "        [ 0.3482, -0.1162]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence1 = [[200, 200, 200]]\n",
        "sequence2 = [[200, 200]]\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id]\n",
        "]\n",
        "tensor1 = torch.tensor(sequence1)\n",
        "print(tensor1)\n",
        "\n",
        "tensor2 = torch.tensor(sequence2)\n",
        "print(tensor2)\n",
        "\n",
        "batched_tensor = torch.tensor(batched_ids)\n",
        "print(batched_tensor)\n",
        "\n",
        "\n",
        "print(model(tensor1).logits)\n",
        "print(model(tensor2).logits)\n",
        "print(model(batched_tensor).logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay_uSfVItCwi",
        "outputId": "6eed7ea9-f4ef-4c9c-c490-b1e43fbbe2b6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[200, 200, 200]])\n",
            "tensor([[200, 200]])\n",
            "tensor([[200, 200, 200],\n",
            "        [200, 200,   0]])\n",
            "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 1.5694, -1.3895],\n",
            "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "attention_tensor = torch.tensor(attention_mask)\n",
        "\n",
        "print(model(batched_tensor, attention_mask=attention_tensor).logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Bh1kUqUvgic",
        "outputId": "0f589519-f3d9-47f9-e5fb-ee5252578077"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.5694, -1.3895],\n",
            "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    }
  ]
}